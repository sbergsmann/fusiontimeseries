{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from fusiontimeseries.lib.get_next_path import get_next_path\n",
    "\n",
    "base_dir = Path(\"./results\")\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_dir = get_next_path(base_fname=\"chronos2-rss-bilinear\", base_dir=base_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=False)\n",
    "print(f\"Output directory created at: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusiontimeseries.lib.config import FTSConfig\n",
    "\n",
    "fts_config = FTSConfig(\n",
    "    op_embedding_dim=512,\n",
    "    num_ops=4,\n",
    "    context_length=512,\n",
    "    pred_tail_timestamps=80,\n",
    "    batch_size=128,\n",
    "    stratification_bins=5,\n",
    "    sampling_bins=5,\n",
    "    val_size=0.1,\n",
    "    padding_value=torch.nan,\n",
    "    padding_mask_default=0.0,\n",
    "    padding_mask_indicator=1.0,\n",
    "    stratification=\"opc_pca\",\n",
    "    sampling_strategy=\"linear\",\n",
    "    data_augmentation=\"white_noise\",\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    lr_scheduler_warmup_ratio=0.0,\n",
    "    optimizer_type=\"adamw_torch_fused\",\n",
    "    max_grad_norm=1.0,\n",
    "    max_steps=4000,\n",
    "    eval_steps=200,\n",
    "    gradient_accumulation_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chronos import Chronos2Model\n",
    "\n",
    "model = Chronos2Model.from_pretrained(\"amazon/chronos-2\")\n",
    "model.chronos_config.context_length = fts_config.context_length\n",
    "model = model.to(fts_config.device)  # type: ignore\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusiontimeseries.modules import ContinuousConditionEmbed\n",
    "\n",
    "shared_p_projection = ContinuousConditionEmbed(\n",
    "    embedding_dim=fts_config.op_embedding_dim,\n",
    "    n_cond=fts_config.num_ops,\n",
    "    max_wavelength=10_000,\n",
    "    init_weights=\"kaiming_uniform\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fusiontimeseries.lib.conditioning import ConditionRegistry\n",
    "from fusiontimeseries.loralib.layers import LoRALayer, layer_dict\n",
    "from fusiontimeseries.loralib.utils import expand_like\n",
    "\n",
    "OP_PARAM_KEY: str = \"op_params\"\n",
    "\n",
    "\n",
    "class RSSBilinearLoRA(nn.Linear, LoRALayer):\n",
    "    # Class-level shared projection (will be set once for all instances)\n",
    "    _shared_p_projection: ContinuousConditionEmbed | None = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        lora_dropout: float = 0.0,\n",
    "        fan_in_fan_out: bool = False,\n",
    "        merge_weights: bool = False,  # keep false due to conditioning\n",
    "        post_layer_norm: bool = False,\n",
    "        pre_batch_norm: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        LoRALayer.__init__(\n",
    "            self,\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            merge_weights=merge_weights,\n",
    "        )\n",
    "\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.post_layer_norm = post_layer_norm\n",
    "        self.pre_batch_norm = pre_batch_norm\n",
    "\n",
    "        self.lora_scale = self.lora_alpha / r\n",
    "        self.p_dim = fts_config.op_embedding_dim  # conditioning dimension\n",
    "\n",
    "        self._init_lora(r)\n",
    "        self.reset_parameters()\n",
    "        if fan_in_fan_out:\n",
    "            self.weight.data = self.weight.data.transpose(0, 1)\n",
    "        if self.post_layer_norm:\n",
    "            self.post_ln = nn.LayerNorm(out_features)\n",
    "            self.merge_weights = False\n",
    "        if self.pre_batch_norm:\n",
    "            self.pre_bn = nn.BatchNorm1d(in_features, affine=False)\n",
    "            self.merge_weights = False\n",
    "\n",
    "    def _init_lora(self, r):\n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, self.in_features)))\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((self.out_features, r)))\n",
    "            self.lora_C = nn.Parameter(self.weight.new_zeros((r, self.p_dim)))\n",
    "            self.lora_S = nn.Parameter(self.weight.new_zeros((r, self.p_dim)))\n",
    "        else:\n",
    "            try:\n",
    "                # ensure parameters do not exist if they are zero\n",
    "                delattr(self, \"lora_A\")\n",
    "                delattr(self, \"lora_B\")\n",
    "                delattr(self, \"lora_C\")\n",
    "                delattr(self, \"lora_S\")\n",
    "                delattr(self, \"lora_scale\")\n",
    "            except AttributeError:\n",
    "                pass\n",
    "        self.weight.requires_grad = False\n",
    "        self.r = r\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.Linear.reset_parameters(self)\n",
    "        # initialize A the same way as the default for nn.Linear and B to zero\n",
    "        # adapt to initialization via PCA on pretrained weights\n",
    "        if hasattr(self, \"lora_C\"):\n",
    "            nn.init.kaiming_uniform_(self.lora_C, a=math.sqrt(5))\n",
    "        if hasattr(self, \"lora_S\"):\n",
    "            nn.init.zeros_(self.lora_S)\n",
    "        if hasattr(self, \"lora_A\"):\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        if hasattr(self, \"lora_B\"):\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def change_lora_rank(self, new_rank):\n",
    "        if new_rank != self.r:\n",
    "            self._init_lora(new_rank)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        def T(w):\n",
    "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
    "\n",
    "        nn.Linear.train(self, mode)\n",
    "        if mode:\n",
    "            if self.merge_weights and self.merged:\n",
    "                # Make sure that the weights are not merged\n",
    "                if self.r > 0:\n",
    "                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.lora_scale\n",
    "                self.merged = False\n",
    "        else:\n",
    "            if self.merge_weights and not self.merged:\n",
    "                # Merge the weights and mark it\n",
    "                if self.r > 0:\n",
    "                    self.weight.data += T(self.lora_B @ self.lora_A) * self.lora_scale\n",
    "                self.merged = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (B, ..., in_features)\n",
    "        \"\"\"\n",
    "\n",
    "        def T(w):\n",
    "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
    "\n",
    "        # Base linear projection\n",
    "        y_base = F.linear(x, T(self.weight), bias=self.bias)  # (B, ..., out_features)\n",
    "        if self.pre_batch_norm:\n",
    "            x = self.pre_bn(x)\n",
    "\n",
    "        if self.r > 0 and not self.merged:\n",
    "            p: torch.Tensor | None = ConditionRegistry.get(OP_PARAM_KEY)\n",
    "            if p is not None and self._shared_p_projection is not None:\n",
    "                p = p.to(self.lora_C.device)\n",
    "\n",
    "                # Learnable representation of p: (B, p_dim)\n",
    "                p_repr: torch.Tensor = self._shared_p_projection(p)\n",
    "\n",
    "                # condition projection: (B, r) = (B, p_dim) @ (r, p_dim).T\n",
    "                c = F.linear(p_repr, self.lora_C)\n",
    "\n",
    "                # rank_space_shift: (B, r) = (B, p_dim) @ (r, p_dim).T\n",
    "                s = F.linear(p_repr, self.lora_S)\n",
    "\n",
    "                # input projection: (B, ..., r) = ( (B, ..., in_features) @ (r, in_features).T )\n",
    "                h = F.linear(self.lora_dropout(x), self.lora_A)\n",
    "\n",
    "                # introduce shift gate\n",
    "                # only activate gate when x activates that rank direction\n",
    "                s_gate = torch.sigmoid(h.norm(dim=-1, keepdim=True))\n",
    "\n",
    "                # Bilinear FiLM-inspired modulation in rank space\n",
    "                # (B, ..., r)\n",
    "                h_mod = h * (\n",
    "                    1.0 + expand_like(target=c, like=h)\n",
    "                ) + s_gate * expand_like(target=s, like=h)\n",
    "\n",
    "                # Project back to output space\n",
    "                # (B, ..., out_features) = ( (B, ..., r) @ (out_features, r).T )\n",
    "                delta_y = F.linear(h_mod, self.lora_B) * self.lora_scale\n",
    "                y_base += delta_y\n",
    "\n",
    "            else:\n",
    "                raise RuntimeError(\n",
    "                    \"Operating parameters not found or _shared_p_projection is None during forward pass.\"\n",
    "                )\n",
    "\n",
    "            if self.post_layer_norm:\n",
    "                y_base = self.post_ln(y_base)\n",
    "\n",
    "        return y_base\n",
    "\n",
    "\n",
    "# Register the custom layer\n",
    "layer_dict[\"RSSBilinearLoRA\"] = RSSBilinearLoRA\n",
    "\n",
    "# Register shared projection\n",
    "RSSBilinearLoRA._shared_p_projection = shared_p_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RSSBilinearLoRA.convert(\n",
    "    module=model,\n",
    "    kind=\"RSSBilinearLoRA\",\n",
    "    lora_rank=8,\n",
    "    lora_alpha=16,\n",
    "    target_module_names=[\n",
    "        \"self_attention.q\",\n",
    "        \"self_attention.k\",\n",
    "        \"self_attention.v\",\n",
    "        \"self_attention.o\",\n",
    "        \"output_patch_embedding.output_layer\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Register the shared projection as a submodule so it gets saved/loaded properly\n",
    "model.shared_condition_projection = shared_p_projection\n",
    "model.shared_condition_projection.to(fts_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusiontimeseries.loralib.utils import mark_only_lora_as_trainable\n",
    "\n",
    "\n",
    "mark_only_lora_as_trainable(model=model, bias=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusiontimeseries.loralib.utils import print_trainable_parameters\n",
    "\n",
    "\n",
    "print_trainable_parameters(model, save_path=output_dir / \"trainable_params.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusiontimeseries.finetuning.chronos2.dataset import Chronos2Dataset\n",
    "\n",
    "train_dataset, val_dataset = Chronos2Dataset.train_val_split(fts_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    per_device_train_batch_size=fts_config.batch_size,\n",
    "    per_device_eval_batch_size=fts_config.batch_size,\n",
    "    learning_rate=fts_config.learning_rate,\n",
    "    lr_scheduler_type=fts_config.lr_scheduler_type,\n",
    "    warmup_ratio=fts_config.lr_scheduler_warmup_ratio,\n",
    "    optim=fts_config.optimizer_type,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=fts_config.eval_steps,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    "    max_steps=fts_config.max_steps,\n",
    "    gradient_accumulation_steps=fts_config.gradient_accumulation_steps,\n",
    "    dataloader_num_workers=0,\n",
    "    tf32=False,\n",
    "    bf16=False,\n",
    "    save_only_model=True,\n",
    "    prediction_loss_only=True,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=fts_config.eval_steps,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=fts_config.eval_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    use_cpu=False,\n",
    "    label_names=[\"future_target\"],\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=fts_config.max_grad_norm,\n",
    ")\n",
    "training_arguments._n_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "from chronos.chronos2.model import Chronos2Model, Chronos2Output\n",
    "\n",
    "\n",
    "class ConditionedTrainer(Trainer):\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model: Chronos2Model,\n",
    "        inputs: dict[str, torch.Tensor],\n",
    "        *args,\n",
    "        return_outputs=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Tensor[B, N]\n",
    "        p_raw: torch.Tensor | None = inputs.pop(\n",
    "            \"operating_parameters\", None\n",
    "        )  # remove before forward, otherwise TypeError in Trainer\n",
    "        assert p_raw is not None, \"operating_parameters key is missing in inputs\"\n",
    "\n",
    "        with ConditionRegistry.patch(op_params=p_raw):\n",
    "            outputs: Chronos2Output = model(**inputs)\n",
    "\n",
    "        loss = outputs.loss if hasattr(outputs, \"loss\") else outputs[0]\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "trainer = ConditionedTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "with open(output_dir / \"training_args.json\", \"w\") as f:\n",
    "    json.dump(trainer.args.to_dict(), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_output = trainer.train()\n",
    "\n",
    "with open(output_dir / \"train_summary.json\", \"w\") as f:\n",
    "    json.dump(train_output._asdict(), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusiontimeseries.loralib.utils import lora_state_dict\n",
    "\n",
    "lora_weights = lora_state_dict(model)\n",
    "torch.save(lora_weights, output_dir / \"lora_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_data = Chronos2Dataset.get_benchmark_flux_traces(fts_config)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusiontimeseries.lib.benchmarking import rmse_with_standard_error\n",
    "from fusiontimeseries.lib.dataset import FluxData\n",
    "import numpy as np\n",
    "\n",
    "START_IDX: int = 80\n",
    "\n",
    "id_benchmark_data = benchmark_data[\"id\"]\n",
    "id_benchmark_forecasts: dict[int, list[float]] = {}\n",
    "for flux_id, flux_data in id_benchmark_data.items():\n",
    "    flux_data: FluxData\n",
    "    energy_flux = np.array(flux_data.energy_flux)\n",
    "    op_params = (\n",
    "        torch.Tensor(flux_data.operating_parameters).unsqueeze(0).to(fts_config.device)\n",
    "    )\n",
    "\n",
    "    ctx: np.ndarray = energy_flux[:START_IDX]\n",
    "    while len(ctx) < len(energy_flux):\n",
    "        with torch.no_grad():\n",
    "            tctx = torch.full(\n",
    "                size=(1, fts_config.context_length), fill_value=fts_config.padding_value\n",
    "            )  # NaN\n",
    "            tctx[0, -len(ctx) :] = torch.tensor(ctx)\n",
    "            context_mask = torch.full_like(\n",
    "                tctx, fill_value=fts_config.padding_mask_default\n",
    "            )  # 0.0\n",
    "            context_mask[0, -len(ctx) :] = fts_config.padding_mask_indicator  # 1.0\n",
    "\n",
    "            with ConditionRegistry.patch(op_params=op_params):\n",
    "                output: Chronos2Output = model(\n",
    "                    context=tctx.to(fts_config.device),\n",
    "                    context_mask=context_mask.to(fts_config.device),\n",
    "                )\n",
    "        if output.quantile_preds is not None:\n",
    "            quantiles: torch.Tensor = output.quantile_preds  # (B, Qs=21, pred_len)\n",
    "            median_quantile: int = quantiles.shape[1] // 2\n",
    "            forecast: np.ndarray = (\n",
    "                quantiles[:, median_quantile, :].cpu().numpy().flatten()\n",
    "            )  # (pred_len,)\n",
    "            ctx = np.concatenate([ctx, forecast])\n",
    "\n",
    "    id_benchmark_forecasts[flux_id] = ctx[\n",
    "        : len(energy_flux)\n",
    "    ].tolist()  # Trim to original length\n",
    "\n",
    "\n",
    "id_benchmark_means: list[np.floating] = [\n",
    "    np.mean(flux_data.energy_flux[:-80]) for _id, flux_data in id_benchmark_data.items()\n",
    "]\n",
    "id_forecast_means: list[np.floating] = [\n",
    "    np.mean(forecast[:-80]) for forecast in id_benchmark_forecasts.values()\n",
    "]\n",
    "id_rmse, id_rmse_se = rmse_with_standard_error(\n",
    "    y_true=np.array(id_benchmark_means), y_pred=np.array(id_forecast_means)\n",
    ")\n",
    "id_rmse, id_rmse_se, id_benchmark_means, id_forecast_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_benchmark_data = benchmark_data[\"ood\"]\n",
    "ood_benchmark_forecasts: dict[int, list[float]] = {}\n",
    "for flux_id, flux_data in ood_benchmark_data.items():\n",
    "    flux_data: FluxData\n",
    "    energy_flux = np.array(flux_data.energy_flux)\n",
    "    op_params = (\n",
    "        torch.Tensor(flux_data.operating_parameters).unsqueeze(0).to(fts_config.device)\n",
    "    )\n",
    "\n",
    "    ctx: np.ndarray = energy_flux[:START_IDX]\n",
    "    while len(ctx) < len(energy_flux):\n",
    "        with torch.no_grad():\n",
    "            tctx = torch.full(\n",
    "                size=(1, fts_config.context_length), fill_value=fts_config.padding_value\n",
    "            )  # NaN\n",
    "            tctx[0, -len(ctx) :] = torch.tensor(ctx)\n",
    "            context_mask = torch.full_like(\n",
    "                tctx, fill_value=fts_config.padding_mask_default\n",
    "            )  # 0.0\n",
    "            context_mask[0, -len(ctx) :] = fts_config.padding_mask_indicator  # 1.0\n",
    "\n",
    "            with ConditionRegistry.patch(op_params=op_params):\n",
    "                output: Chronos2Output = model(\n",
    "                    context=tctx.to(fts_config.device),\n",
    "                    context_mask=context_mask.to(fts_config.device),\n",
    "                )\n",
    "        if output.quantile_preds is not None:\n",
    "            quantiles: torch.Tensor = output.quantile_preds  # (B, Qs=21, pred_len)\n",
    "            median_quantile: int = quantiles.shape[1] // 2\n",
    "            forecast: np.ndarray = (\n",
    "                quantiles[:, median_quantile, :].cpu().numpy().flatten()\n",
    "            )  # (pred_len,)\n",
    "            ctx = np.concatenate([ctx, forecast])\n",
    "\n",
    "    ood_benchmark_forecasts[flux_id] = ctx[\n",
    "        : len(energy_flux)\n",
    "    ].tolist()  # Trim to original length\n",
    "\n",
    "ood_benchmark_means: list[np.floating] = [\n",
    "    np.mean(flux_data.energy_flux[:-80])\n",
    "    for _id, flux_data in ood_benchmark_data.items()\n",
    "]\n",
    "ood_forecast_means: list[np.floating] = [\n",
    "    np.mean(forecast[:-80]) for forecast in ood_benchmark_forecasts.values()\n",
    "]\n",
    "ood_rmse, ood_rmse_se = rmse_with_standard_error(\n",
    "    y_true=np.array(ood_benchmark_means), y_pred=np.array(ood_forecast_means)\n",
    ")\n",
    "ood_rmse, ood_rmse_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save forecast means for each time series id and the total rmse and standard error for id and ood to the load_dir in a json file\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    \"metrics\": {\n",
    "        \"id\": {\n",
    "            \"rmse\": float(id_rmse),\n",
    "            \"standard_error\": float(id_rmse_se),\n",
    "        },\n",
    "        \"ood\": {\n",
    "            \"rmse\": float(ood_rmse),\n",
    "            \"standard_error\": float(ood_rmse_se),\n",
    "        },\n",
    "    },\n",
    "    \"forecasts\": {\n",
    "        \"id\": {**id_benchmark_forecasts},\n",
    "        \"ood\": {**ood_benchmark_forecasts},\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(output_dir / \"benchmark_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(3 * 3, 2 * 2), sharex=\"col\")\n",
    "axes = axes.flatten()\n",
    "for ax_id, flux_id in enumerate(id_benchmark_data.keys()):\n",
    "    ax = axes[ax_id]\n",
    "    flux_data = id_benchmark_data[flux_id]\n",
    "    energy_flux = np.array(flux_data.energy_flux)\n",
    "\n",
    "    ax.plot(energy_flux)\n",
    "    ax.plot(id_benchmark_forecasts[flux_id])\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        0.9,\n",
    "        f\"OPs: {flux_data.operating_parameters}\",\n",
    "        horizontalalignment=\"center\",\n",
    "        verticalalignment=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=7,\n",
    "    )\n",
    "    ax.set_title(f\"Flux ID: {flux_id}\", fontdict={\"fontsize\": 5})\n",
    "    ax.set_xlabel(\"Time Step\", fontdict={\"fontsize\": 5})\n",
    "    ax.set_ylabel(\"Energy Flux\", fontdict={\"fontsize\": 5})\n",
    "fig.savefig(output_dir / \"id_benchmark_forecasts.png\")\n",
    "fig.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(3 * 3, 2 * 2), sharex=\"col\")\n",
    "axes = axes.flatten()\n",
    "for ax_id, flux_id in enumerate(ood_benchmark_data.keys()):\n",
    "    ax = axes[ax_id]\n",
    "    flux_data = ood_benchmark_data[flux_id]\n",
    "    energy_flux = np.array(flux_data.energy_flux)\n",
    "\n",
    "    ax.plot(energy_flux)\n",
    "    ax.plot(ood_benchmark_forecasts[flux_id])\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        0.9,\n",
    "        f\"OPs: {flux_data.operating_parameters}\",\n",
    "        horizontalalignment=\"center\",\n",
    "        verticalalignment=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=7,\n",
    "    )\n",
    "    ax.set_title(f\"Flux ID: {flux_id}\", fontdict={\"fontsize\": 5})\n",
    "    ax.set_xlabel(\"Time Step\", fontdict={\"fontsize\": 5})\n",
    "    ax.set_ylabel(\"Energy Flux\", fontdict={\"fontsize\": 5})\n",
    "fig.savefig(output_dir / \"ood_benchmark_forecasts.png\")\n",
    "fig.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusiontimeseries (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
