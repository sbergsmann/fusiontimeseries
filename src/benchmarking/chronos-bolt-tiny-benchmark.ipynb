{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Chronos Time Series Prediction Playbook\n",
    "\n",
    "[Huggingface](https://huggingface.co/amazon/chronos-bolt-tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_path():\n",
    "    \"\"\"Extend notebooks system path config to import relative packages.\"\"\"\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    parent_folder = str(Path.cwd().parent)\n",
    "    print(f\"Adding {parent_folder} to sys.path\")\n",
    "    if parent_folder not in sys.path:\n",
    "        sys.path.insert(0, parent_folder)\n",
    "\n",
    "\n",
    "extend_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from chronos import BaseChronosPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from services import PredictionVizualizationProvider\n",
    "\n",
    "predviz_provider = PredictionVizualizationProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "MODEL_PATH = \"amazon/chronos-bolt-tiny\"\n",
    "N_TIMESERIES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline: BaseChronosPipeline = BaseChronosPipeline.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_PATH,\n",
    "    device_map=DEVICE,  # use \"cpu\" for CPU inference and \"mps\" for Apple Silicon\n",
    "    dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"Evaluator class for computing forecasting metrics.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def mae(pred: torch.Tensor, tgt: torch.Tensor) -> float:\n",
    "        \"\"\"Compute Mean Absolute Error (MAE).\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted values tensor.\n",
    "            tgt (torch.Tensor): Target values tensor.\n",
    "\n",
    "        Returns:\n",
    "            float: MAE value.\n",
    "        \"\"\"\n",
    "        return torch.mean(torch.abs(pred - tgt)).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def rmse(pred: torch.Tensor, tgt: torch.Tensor) -> float:\n",
    "        \"\"\"Compute Root Mean Squared Error (RMSE).\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted values tensor.\n",
    "            tgt (torch.Tensor): Target values tensor.\n",
    "\n",
    "        Returns:\n",
    "            float: RMSE value.\n",
    "        \"\"\"\n",
    "        return torch.sqrt(torch.mean((pred - tgt) ** 2)).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def nrmse(pred: torch.Tensor, tgt: torch.Tensor) -> float:\n",
    "        \"\"\"Compute Normalized Root Mean Squared Error (NRMSE).\n",
    "\n",
    "        Normalized by the range of the target (max - min).\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted values tensor.\n",
    "            tgt (torch.Tensor): Target values tensor.\n",
    "\n",
    "        Returns:\n",
    "            float: NRMSE value.\n",
    "        \"\"\"\n",
    "        rmse_val = torch.sqrt(torch.mean((pred - tgt) ** 2))\n",
    "        tgt_range = torch.max(tgt) - torch.min(tgt)\n",
    "        if tgt_range == 0:\n",
    "            return float(\"inf\")\n",
    "        return (rmse_val / tgt_range).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def nd(pred: torch.Tensor, tgt: torch.Tensor) -> float:\n",
    "        \"\"\"Compute Normalized Deviation (ND).\n",
    "\n",
    "        Sum of absolute errors divided by sum of absolute target values.\n",
    "        Also known as WAPE (Weighted Absolute Percentage Error).\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted values tensor.\n",
    "            tgt (torch.Tensor): Target values tensor.\n",
    "\n",
    "        Returns:\n",
    "            float: ND value.\n",
    "        \"\"\"\n",
    "        sum_abs_error = torch.sum(torch.abs(pred - tgt))\n",
    "        sum_abs_tgt = torch.sum(torch.abs(tgt))\n",
    "        if sum_abs_tgt == 0:\n",
    "            return float(\"inf\")\n",
    "        return (sum_abs_error / sum_abs_tgt).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def mape(pred: torch.Tensor, tgt: torch.Tensor) -> float:\n",
    "        \"\"\"Compute Mean Absolute Percentage Error (MAPE).\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted values tensor.\n",
    "            tgt (torch.Tensor): Target values tensor.\n",
    "\n",
    "        Returns:\n",
    "            float: MAPE value as percentage.\n",
    "        \"\"\"\n",
    "        # Avoid division by zero by adding small epsilon where tgt is zero\n",
    "        epsilon = 1e-8\n",
    "        safe_tgt = torch.where(tgt == 0, epsilon, tgt)\n",
    "        return (100.0 * torch.abs(pred - tgt) / torch.abs(safe_tgt)).mean().item()\n",
    "\n",
    "    @staticmethod\n",
    "    def smape(pred: torch.Tensor, tgt: torch.Tensor) -> float:\n",
    "        \"\"\"Compute Symmetric Mean Absolute Percentage Error (sMAPE).\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted values tensor.\n",
    "            tgt (torch.Tensor): Target values tensor.\n",
    "\n",
    "        Returns:\n",
    "            float: sMAPE value.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            (100.0 * torch.abs(pred - tgt) / ((torch.abs(tgt) + torch.abs(pred)) / 2))\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def mase(\n",
    "        pred: torch.Tensor, tgt: torch.Tensor, context: torch.Tensor | None = None\n",
    "    ) -> float:\n",
    "        \"\"\"Compute Mean Absolute Scaled Error (MASE).\n",
    "\n",
    "        Uses naive forecast on context (history) as benchmark if available,\n",
    "        otherwise uses naive forecast on target (less robust).\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted values tensor.\n",
    "            tgt (torch.Tensor): Target values tensor.\n",
    "            context (torch.Tensor | None): Context/History tensor.\n",
    "\n",
    "        Returns:\n",
    "            float: MASE value.\n",
    "        \"\"\"\n",
    "        mae_pred = torch.mean(torch.abs(pred - tgt))\n",
    "\n",
    "        if context is not None and context.numel() > 1:\n",
    "            # Use context (history) for naive error scale\n",
    "            # Calculate mean absolute difference of the context (in-sample naive error)\n",
    "            ctx = context.squeeze()\n",
    "            if ctx.ndim > 1:\n",
    "                ctx = ctx.view(-1)\n",
    "            scale = torch.mean(torch.abs(ctx[1:] - ctx[:-1]))\n",
    "        else:\n",
    "            # Fallback: use target (less robust as it's out-of-sample)\n",
    "            naive_pred = tgt[:-1]\n",
    "            naive_tgt = tgt[1:]\n",
    "            scale = torch.mean(torch.abs(naive_pred - naive_tgt))\n",
    "\n",
    "        if scale == 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        return (mae_pred / scale).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def directional_accuracy(pred: torch.Tensor, tgt: torch.Tensor) -> float:\n",
    "        \"\"\"Compute Directional Accuracy (percentage of correct direction predictions).\n",
    "\n",
    "        Measures if the predicted direction of change matches the actual direction.\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted values tensor.\n",
    "            tgt (torch.Tensor): Target values tensor.\n",
    "\n",
    "        Returns:\n",
    "            float: Directional accuracy as percentage (0-100).\n",
    "        \"\"\"\n",
    "        if len(pred) < 2 or len(tgt) < 2:\n",
    "            return 0.0\n",
    "        pred_changes = torch.sign(pred[1:] - pred[:-1])\n",
    "        tgt_changes = torch.sign(tgt[1:] - tgt[:-1])\n",
    "        correct = torch.sum(pred_changes == tgt_changes).item()\n",
    "        total = len(pred_changes)\n",
    "        return (correct / total) * 100.0\n",
    "\n",
    "\n",
    "class Utils:\n",
    "    @staticmethod\n",
    "    def median_forecast(forecast: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute the median forecast from the forecast tensor.\n",
    "\n",
    "        Args:\n",
    "            forecast (torch.Tensor): Forecast tensor of shape [N, prediction_length, n_quantiles].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Median forecast tensor of shape [N, prediction_length].\n",
    "        \"\"\"\n",
    "        n_quantiles = forecast.shape[-1]\n",
    "        median_index = n_quantiles // 2\n",
    "        return forecast[:, :, median_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Generator\n",
    "\n",
    "\n",
    "class FluxTrace:\n",
    "    \"\"\"A class to iterate over time series data in sliding windows for forecasting.\n",
    "\n",
    "    This class allows generating context and target pairs from a time series trace\n",
    "    for training or evaluating forecasting models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        trace: torch.Tensor,\n",
    "        prediction_length: int,\n",
    "        context_length: int | None = None,\n",
    "        window: int | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the FluxTrace iterator.\n",
    "\n",
    "        Args:\n",
    "            trace (torch.Tensor): The full time series data tensor.\n",
    "            prediction_length (int): Length of the prediction horizon.\n",
    "            context_length (int | None): Length of the context window. If None,\n",
    "                defaults to trace length minus prediction_length.\n",
    "            window (int | None): Step size for sliding the window. If None,\n",
    "                defaults to context_length (non-overlapping windows).\n",
    "        \"\"\"\n",
    "        self.trace = trace\n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length\n",
    "        self.window = window\n",
    "        self.metrics: list[dict[str, Any]] = []\n",
    "\n",
    "    def __iter__(self) -> Generator[tuple[torch.Tensor, torch.Tensor, int], None, None]:\n",
    "        \"\"\"Iterate over the trace yielding context and target pairs.\n",
    "\n",
    "        Yields:\n",
    "            tuple[torch.Tensor, torch.Tensor]: A tuple of (context, target) tensors.\n",
    "                - context: Tensor of shape [1, context_length] for model input.\n",
    "                - target: Tensor of shape [prediction_length] for ground truth.\n",
    "        \"\"\"\n",
    "        trace_length = self.trace.shape[-1]\n",
    "        context_length = self.context_length or trace_length - self.prediction_length\n",
    "        start = 0\n",
    "        end = context_length\n",
    "        while end + self.prediction_length <= trace_length:\n",
    "            yield (\n",
    "                self.trace[..., start:end].unsqueeze(0),\n",
    "                self.trace[..., end : end + self.prediction_length],\n",
    "                start,\n",
    "            )\n",
    "            start += self.window or context_length\n",
    "            end += self.window or context_length\n",
    "\n",
    "    def record(\n",
    "        self,\n",
    "        forecast: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        context: torch.Tensor | None = None,\n",
    "        metadata: dict[str, Any] = {},\n",
    "    ) -> None:\n",
    "        \"\"\"Record evaluation metrics for a given forecast and ground truth.\n",
    "\n",
    "        Args:\n",
    "            forecast (torch.Tensor): Forecasted values tensor.\n",
    "            target (torch.Tensor): Ground truth values tensor.\n",
    "            context (torch.Tensor | None): Context values tensor (history).\n",
    "            metadata (dict[str, Any]): Additional metadata for the evaluation. Defaults to empty dict.\n",
    "        \"\"\"\n",
    "        mae = Evaluator.mae(forecast, target)\n",
    "        rmse = Evaluator.rmse(forecast, target)\n",
    "        nrmse = Evaluator.nrmse(forecast, target)\n",
    "        nd = Evaluator.nd(forecast, target)\n",
    "        mape = Evaluator.mape(forecast, target)\n",
    "        smape = Evaluator.smape(forecast, target)\n",
    "        mase = Evaluator.mase(forecast, target, context)\n",
    "        directional_acc = Evaluator.directional_accuracy(forecast, target)\n",
    "\n",
    "        self.metrics.append(\n",
    "            {\n",
    "                \"MAE\": mae,\n",
    "                \"RMSE\": rmse,\n",
    "                \"NRMSE\": nrmse,\n",
    "                \"ND\": nd,\n",
    "                \"MAPE\": mape,\n",
    "                \"sMAPE\": smape,\n",
    "                \"MASE\": mase,\n",
    "                \"Directional Accuracy\": directional_acc,\n",
    "                \"metadata\": metadata,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def forecast_summary(self) -> dict[str, float]:\n",
    "        \"\"\"Compute average metrics over all recorded forecasts.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, float]: A dictionary with average metrics.\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            \"MAE\": 0.0,\n",
    "            \"RMSE\": 0.0,\n",
    "            \"NRMSE\": 0.0,\n",
    "            \"ND\": 0.0,\n",
    "            \"MAPE\": 0.0,\n",
    "            \"sMAPE\": 0.0,\n",
    "            \"MASE\": 0.0,\n",
    "            \"Directional Accuracy\": 0.0,\n",
    "        }\n",
    "        n = len(self.metrics)\n",
    "        if n == 0:\n",
    "            return summary\n",
    "\n",
    "        for metric in self.metrics:\n",
    "            summary[\"MAE\"] += metric[\"MAE\"]\n",
    "            summary[\"RMSE\"] += metric[\"RMSE\"]\n",
    "            summary[\"NRMSE\"] += metric[\"NRMSE\"]\n",
    "            summary[\"ND\"] += metric[\"ND\"]\n",
    "            summary[\"MAPE\"] += metric[\"MAPE\"]\n",
    "            summary[\"sMAPE\"] += metric[\"sMAPE\"]\n",
    "            summary[\"MASE\"] += metric[\"MASE\"]\n",
    "            summary[\"Directional Accuracy\"] += metric[\"Directional Accuracy\"]\n",
    "\n",
    "        for key in summary:\n",
    "            summary[key] /= n\n",
    "\n",
    "        return summary\n",
    "\n",
    "\n",
    "class Scaler:\n",
    "    \"\"\"Utility class for normalizing and denormalizing time series data.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def setnorm(ctx: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Normalize the context tensor using z-score normalization.\n",
    "\n",
    "        Args:\n",
    "            ctx (torch.Tensor): Input context tensor.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                - normed: Normalized tensor.\n",
    "                - mean: Mean tensor used for normalization.\n",
    "                - std: Standard deviation tensor used for normalization.\n",
    "        \"\"\"\n",
    "        mean = ctx.mean(dim=-1, keepdim=True)\n",
    "        std = ctx.std(dim=-1, keepdim=True) + 1e-8\n",
    "        normed = (ctx - mean) / std\n",
    "        return normed, mean, std\n",
    "\n",
    "    @staticmethod\n",
    "    def denorm(\n",
    "        pred: torch.Tensor, mean: torch.Tensor, std: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Denormalize the prediction tensor back to original scale.\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Normalized prediction tensor.\n",
    "            mean (torch.Tensor): Mean tensor from normalization.\n",
    "            std (torch.Tensor): Standard deviation tensor from normalization.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Denormalized prediction tensor.\n",
    "        \"\"\"\n",
    "        return pred * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FluxType:\n",
    "    ELECTRON_FLUX: int = 0\n",
    "    ENERGY_FLUX: int = 1\n",
    "    ION_FLUX: int = 2\n",
    "\n",
    "\n",
    "class FluxTraceProvider:\n",
    "    FLUX_TYPE: FluxType = FluxType()\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir: Path,\n",
    "        filename_convention: str = \"fluxes_{iteration}.dat\",\n",
    "    ) -> None:\n",
    "        \"\"\"Data Access Provider for flux traces\n",
    "\n",
    "        Args:\n",
    "            dir (Path): The path to the directory where the flux traces are stored\n",
    "            filename_convention (str, optional): The filename convention. Defaults to \"fluxes_{iteration}.dat\".\n",
    "        \"\"\"\n",
    "        self.dir = dir\n",
    "        self.filename_convention = filename_convention\n",
    "\n",
    "    @cache\n",
    "    def load_flux_energy_data(self, iteration: int) -> torch.Tensor:\n",
    "        file_path: Path = self.dir / self.filename_convention.format(\n",
    "            iteration=iteration\n",
    "        )\n",
    "        data: np.ndarray = np.loadtxt(file_path)\n",
    "        return torch.from_numpy(data[:, self.FLUX_TYPE.ENERGY_FLUX]).to(\n",
    "            self.DEVICE\n",
    "        )  # return only energy fluxes\n",
    "\n",
    "    @cache\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get the number of flux trace files available.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of flux trace files.\n",
    "        \"\"\"\n",
    "        return len(os.listdir(self.dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class FluxDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for flux traces using FluxTraceProvider.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        provider: FluxTraceProvider,\n",
    "        prediction_length: int,\n",
    "        context_length: int | None = None,\n",
    "        window: int | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            provider (FluxTraceProvider): The data provider.\n",
    "            prediction_length (int): Length of prediction horizon.\n",
    "            context_length (int | None): Length of context window.\n",
    "            window (int | None): Step size for sliding window.\n",
    "        \"\"\"\n",
    "        self.provider: FluxTraceProvider = provider\n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length\n",
    "        self.window = window\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.provider)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> FluxTrace:\n",
    "        trace = self.provider.load_flux_energy_data(idx)\n",
    "        flux_trace = FluxTrace(\n",
    "            trace=trace,\n",
    "            prediction_length=self.prediction_length,\n",
    "            context_length=self.context_length,\n",
    "            window=self.window,\n",
    "        )\n",
    "        return flux_trace\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# dataset = FluxDataset(\n",
    "#     provider=fluxtrace_provider,\n",
    "#     iterations=[0, 1, 2],\n",
    "#     prediction_length=PREDICTION_LEN,\n",
    "#     context_length=CONTEXT_LEN,\n",
    "#     window=10,\n",
    "# )\n",
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Any\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "class Benchmark(BaseModel):\n",
    "    model: str\n",
    "    prediction_length: int\n",
    "    context_length: int | None = None\n",
    "    window: int | None = None\n",
    "    benchmark_start_timestamp: float | None = Field(\n",
    "        default=None, description=\"Posix Timestamp.\"\n",
    "    )\n",
    "    benchmark_end_timestamp: float | None = Field(\n",
    "        default=None, description=\"Posix Timestamp.\"\n",
    "    )\n",
    "    metrics: list[dict[str, Any]] = []\n",
    "\n",
    "\n",
    "class FluxForecastingBenchmarker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: FluxDataset,\n",
    "        model: str,\n",
    "        save_dir: Path = Path(\".\").resolve().parent / \"data\" / \"benchmarks\",\n",
    "        benchmark_file_convention: str = \"{timestamp}_benchmark_{model}.json\",\n",
    "    ) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        self.benchmark_file_convention = benchmark_file_convention\n",
    "        self.save_dir = save_dir\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Benchmarker will save results to: {self.save_dir}\")\n",
    "\n",
    "        self.metrics: list[dict[str, Any]] = []\n",
    "        self.benchmark_start_time: datetime | None = None\n",
    "        self.benchmark_end_time: datetime | None = None\n",
    "\n",
    "    def benchmark(self, batch_size: int = 1, stop_after: int | None = None) -> None:\n",
    "        # reset\n",
    "        self.metrics = []\n",
    "        self.benchmark_start_time = None\n",
    "        self.benchmark_end_time = None\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            self.dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x[0]\n",
    "        )\n",
    "\n",
    "        self.benchmark_start_time = datetime.now()\n",
    "        for idx, fluxtrace in tqdm(enumerate(dataloader)):\n",
    "            fluxtrace: FluxTrace\n",
    "            if stop_after is not None and idx >= stop_after:\n",
    "                break\n",
    "\n",
    "            for ctx, tgt, _ in fluxtrace:\n",
    "                ctx: torch.Tensor\n",
    "                tgt: torch.Tensor\n",
    "                _: int\n",
    "\n",
    "                # Normalize context\n",
    "                normed_ctx, mean, std = Scaler.setnorm(ctx)\n",
    "\n",
    "                # Generate forecast\n",
    "                with torch.no_grad():\n",
    "                    forecast: torch.Tensor = pipeline.predict(\n",
    "                        normed_ctx,\n",
    "                        prediction_length=self.dataset.prediction_length,\n",
    "                    )\n",
    "                    forecast = forecast.permute(0, 2, 1)\n",
    "\n",
    "                # Denormalize forecast\n",
    "                denormed_forecast = Scaler.denorm(forecast, mean, std)\n",
    "\n",
    "                fluxtrace.record(\n",
    "                    forecast=Utils.median_forecast(denormed_forecast).squeeze(0),\n",
    "                    target=tgt.squeeze(0),\n",
    "                    context=ctx.squeeze(0),\n",
    "                )\n",
    "\n",
    "            self.metrics.append(fluxtrace.forecast_summary())\n",
    "        self.benchmark_end_time = datetime.now()\n",
    "\n",
    "    def save_benchmark(self) -> None:\n",
    "        \"\"\"Save the benchmark to a file.\"\"\"\n",
    "        benchmark: Benchmark = Benchmark(\n",
    "            model=self.model,\n",
    "            prediction_length=self.dataset.prediction_length,\n",
    "            context_length=self.dataset.context_length,\n",
    "            window=self.dataset.window,\n",
    "            benchmark_start_timestamp=self.benchmark_start_time.timestamp()\n",
    "            if self.benchmark_start_time\n",
    "            else None,\n",
    "            benchmark_end_timestamp=self.benchmark_end_time.timestamp()\n",
    "            if self.benchmark_end_time\n",
    "            else None,\n",
    "            metrics=self.metrics,\n",
    "        )\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        safe_model = self.model.replace(\"/\", \"_\")\n",
    "        filename = self.benchmark_file_convention.format(\n",
    "            model=safe_model, timestamp=timestamp\n",
    "        )\n",
    "\n",
    "        with open(self.save_dir / filename, \"w\") as f:\n",
    "            json.dump(\n",
    "                benchmark.model_dump(exclude_none=True, exclude_defaults=True),\n",
    "                f,\n",
    "                indent=4,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "fluxtrace_provider = FluxTraceProvider(\n",
    "    dir=Path(\".\").resolve().parent.parent / \"data\" / \"flux\" / \"raw\"\n",
    ")\n",
    "dataset = FluxDataset(\n",
    "    provider=fluxtrace_provider,\n",
    "    prediction_length=64,\n",
    "    context_length=128,\n",
    "    window=32,\n",
    ")\n",
    "benchmarker = FluxForecastingBenchmarker(dataset, model=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarker.benchmark(stop_after=10)\n",
    "benchmarker.save_benchmark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusiontimeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
